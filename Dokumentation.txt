Pong ist ein Zweispieler-Spiel, bei dem jeder Spieler einen "Schläger" nutzt, um einen Ball zurückzuschlagen und Punkte zu erzielen. Das Spiel simuliert den Tischtennissport. In diesem Projekt übernimmt ein Bot die Rolle des Gegners, der mithilfe von künstlicher Intelligenz, Q-Learning und Reinforcement Learning entwickelt wurde. Ich habe dieses Spiel gewählt, da es recht bekannt ist und die Bewegungen und die Umsetzung leicht zu verstehen sind.

Um den Bot herausfordernder zu machen, wird er mit Reinforcement Learning trainiert. Dabei lernt der Agent, in seiner Umgebung optimale Entscheidungen zu treffen, um eine Belohnung zu maximieren. Wenn der Agent einen Fehler macht oder eine schlechte Entscheidung trifft, wird er bestraft. Das Prinzip basiert darauf, durch Versuch und Irrtum zu lernen und sich dadurch zu verbessern.


Der Agent ist ein autonomes System, das in einer Umgebung agiert und Entscheidungen trifft. Er verhält sich wie ein Spieler, der eine Aktion in einem bestimmten Zustand (state) ausführt. Aktionen sind alle möglichen Entscheidungen, die dem Agenten zur Auswahl stehen. Je nach dem ob er eine vorteilhafte Aktion ausführt, bekommt der Agent einen Reward. Wenn er aber eine nachteilige Aktion ausführt, kriegt er ein negatives Feedback.


Der Code für das Pong-Spiel wurde mit Hilfe von ChatGPT generiert. Da das Spiel ursprünglich für zwei Spieler gedacht ist, musste ich einen Bot programmieren, der die Bewegungen des Gegners übernimmt. Das bot_pad wird so gesteuert, dass es dem Ball folgt. Wenn der Ball unterhalb des Paddles ist, wird das Paddle nach oben bewegt, um sich der Ballposition anzunähern. Das geschieht ebenfalls, wenn sich der Ball nach unten bewegt. Dadurch ist es unmöglich, den Bot zu schlagen. (Line 68-72)

Das Ziel von Q-Learning in diesem Spiel ist es, dass der Bot durch das Treffen des Balls lern. Die Methode choose_action ist verantwortlich für die Auswahl der nächsten Aktion des Agents basierend auf dem aktuellen Zustand (state) und der Explorationsrate (epsilon). Zunächst prüft die Methode, ob der aktuelle Zustand bereits in der Q-Tabelle vorhanden ist. Falls nicht, wird ein Q-Wert hinzugefügt. Danach wird mit der Methode entschieden, ob der Agent eine zufällige Aktion (Exploration) oder die beste Aktion (Exploitation) auswählt- Das geschieht mithilfe eines Zufallswertes und der gegebenen Explorationsrate (epsilon). Wenn der Zufallswert kleiner als epsilon ist, wird eine zufällige Aktion gewählt, ansonsten wird die Aktion mit dem höchsten Q-Wert für diesen Zustand gewählt. (Line 88, 93)

Um die Q-Tabelle nach jeder Aktion des Agenten zu aktualisieren, wird der Q-Wert des aktuellen Zustands und der ausgeführten Aktion mit einem reward und den möglichen zukünftigen Q-Werten angepasst. Wenn der Agent einen neuen Zustand erreicht, prüft die Methode, ob der aktuelle als auch der nächste Zustand in der Q-Table existieren. Der Q-Wert für den aktuellen state und die gewählte action wird mit der Formel des Q-Learnings aktualisiert: Q(s, a) ← Q(s, a) + α [r + γ maxa′Q(s′, a′) − Q(s, a)]. Der LEARNING_RATE bestimmt, wie stark die Q-Werte aktualisiert werden. (Line 95 - 105)

Mit load_q_table(self) ermöglicht es dem Agenten, seine erlernten Werte fortzusetzen, anstatt jedes Mal von vorne zu beginnen. Das vorhandene Q-Table wird dabei mit numpy gespeichert. (Line 107 - 118)

Der Explorationsrate "EPSILON" bestimmt, wie oft der Agent zufällige Aktionen (Exploration) wählt, statt auf die beste Aktion (Exploitation). In diesem Fall ist der Wert auf 0.1 gesetzt. Das bedeutet, dass der Agent in 10% der Fälle eine zufällige Aktion wählt, um neue Strategien zu erkunden. Die Restlichen 90% wählt er die Aktion mit den höchsten Q-Wert basierend auf seinen bisherigen Erfahrungen. (Line 169)

Der Agent wird über mehrere EPISODES trainiert. Episodes entspricht einem vollständigen Durchlauf des Spiels. Hier wird 1000 Episoden gewählt da dies den Lernprozess für den Agenten erleichtert, ohne Millionen von Runden spielen zu müssen. (Line 78)

Für jede Episode wird der Agent nun eine Aktion auswählen, indem er die Methode choose_action aufruft. Dabei wird der game_state und der Wert von Epsilon übergeben. Der Agent entscheidet dann, ob er eine Exploration oder Exploitation ausführt.

Nachdem der Agent seine Aktion ausgeführt hat, wird die Funkntion game_step aufgerufen, die die Spielumgebung aktualisiert. Der Ball bewegt sich weiter basierend auf seiner Geschwindigkeit. Wenn der Ball mit einem Paddle oder der Grenze des Spielfelds kollidiert, wird dies behandelt.
Der Score für den linken und rechten Spieler wird überprüft, und den reward für den Agenten wird festgelegt, basierend auf den Ergebnissen der aktuellen Situation. Z.B. kann der Agent eine positive Belohnung erhalten, wenn er einen Punk erzielt. Oder eine negative Belohung, wenn der Gegner einen Punkt erzielt.

Anschliessen wird die Q-Tabelle mit der Methode update_q_table aktualisiert. Der Agent benutzt die Belohnung, die er für die positive Aktion erhalten hat, sowie den nächsten Zustand, um den Wert der gewählten Aktion im aktuellen Zustand zu aktualisieren.. Dieser Prozess beruht auf der Q-Learning-Formel und hilft dem Agenten seine Leistung zu verbessern. 

Die Hyperparamter Epsilon, Learning_Rate, Discount_factor und Episodes wurden so gewählt, damit sie nicht während des Lernzprozesses angepasst werden müssen. 
Epsilon bestimmt, ob der Agent eine Exploration auswählt oder auf Exploation, welche bereits in der Q-Table gespeichert sind. Der Wert beträgt 0,1, was bedeutet, dass der Agent exploiated, also mehr aus der bestehenden Erfarhrunen lernt.
Der Learning_Rate steuert, wie stark die Q-Werte in der Q-Table bei jeder Aktualisierung angepasst werden. Der Wert von 0.1 sorgt dafür, dass die Q-Table langsamer angepasst wird und der Agent vorsichtiger lernt.
Der Discount_factor gibt an, wie die zukünftigen Belohnungen im Vergleich zu den aktuellen Belohnungen gewichtet werden. Wenn der Wert hoch ist, berücksichtigt der Agent stark auf die Belohnungen. Wenn er aber niedrig ist, berücksichtig er nur sofortige Belohnungen.
Episodes sind die Anzahl der Trainingsdurchläufe. Hier beträgt der Wert 1000, was bedeutet, dass es 1000 mal durchlaufen wird und daneben neue Erfahrungen sammelt und seine Q-Table aktualisiert. Episodes erlauben es den Agenten ganze Erfahrungen von Anfang bis Ende zu sammeln. Sonst hat es keine klare Struktur und das Lernen würde chaotisch verlaufen.


Ich fand das Thema besonders spannend, da KI derzeit sehr populär ist. Da ich bereits in der 4. Klasse mit Maschinellem Learning durchgenommen habe, waren mir einige Konzepte bekannt. Trotzdem konnte ich mein Wissen vertiefen. Ich entschied mich, mit Turtle zu arbeiten, weil bei mir pygame nicht funktionierte und ich aufgrund von Zeitmangel Turtle auswählte. Der Unterschied zwischen den beiden liegt darin, dass Turtle den Code grafisch darstellt. Zu Beginn hatte ich keine grossen Probleme, aber als es darum ging, Q-Learning oder die Q-Tabelle zu programmieren, stieß ich auf Schwierigkeiten. Ich versuchte es selbst und nutzte Internetquellen, aber es war nicht erfolgreich. Danach half mir Morris, indem er mir seinen Code zur Verfügung stellte und anpasste. Er erklärte mir die Funktionsweise im Detail und half mir, Fehler zu korrigieren. Es gibt immer noch einige Fehler im Code, die ich zu Hause zu lösen versuchte. Manchmal ist es mir gelungen, manchmal nicht und habe es so gelassen, wie er ist. 
